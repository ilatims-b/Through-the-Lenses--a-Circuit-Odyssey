{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformer-lens datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from transformer_lens import HookedTransformer\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"...\"\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/gemma-2-2b-it\"  # Or any supported model; replace with your desired model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_EPOCHS = 2\n",
    "CHECKPOINT_EVERY = 1  # Save probe checkpoint every N epochs\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE, torch_dtype=torch.float16, center_unembed=False)\n",
    "\n",
    "# Freeze model parameters & set to eval mode\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset on which to train tunedlens probes\n",
    "NUM_EXAMPLES = 39470\n",
    "dataset = load_dataset(\"allenai/c4\", name=\"en\", split=\"train\", streaming=True)\n",
    "text_samples = []\n",
    "for example in dataset:\n",
    "    text = example.get(\"text\", \"\").strip()\n",
    "    if text:\n",
    "        text_samples.append(text)\n",
    "    if len(text_samples) >= NUM_EXAMPLES:\n",
    "        break\n",
    "print(f\"Loaded {len(text_samples)} clean text samples\")\n",
    "\n",
    "# Join all text into one long sequence of tokens\n",
    "joined_text = \"\\n\\n\".join(text_samples) \n",
    "all_token_ids = tokenizer.encode(joined_text, return_tensors='pt')[0]  # shape: (total_tokens,)\n",
    "print(f\"Token sequence length: {len(all_token_ids)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special strings which will be added to the start and end of each text sample to follow the transformer model's expected input format\n",
    "#we train the probes on dataset samples with these special tokens added, as we would want to use the lens on examples with the tokens. \n",
    "#In the original tuned lens implementation, \n",
    "prefix_text = \"<start_of_turn>user\\n\"\n",
    "suffix_text = \"<end_of_turn>\"\n",
    "\n",
    "# Encode prefix and suffix tokens with the tokenizer:\n",
    "prefix_tokens = tokenizer.encode(prefix_text, add_special_tokens=False)  # e.g. [....]\n",
    "suffix_tokens = tokenizer.encode(suffix_text, add_special_tokens=False)  # e.g. [....]\n",
    "\n",
    "print(f\"Prefix tokens count: {len(prefix_tokens)}\")\n",
    "print(f\"Suffix tokens count: {len(suffix_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128\n",
    "SUBSET_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_tokens(token_ids, seq_len):\n",
    "    examples = []\n",
    "    total_tokens = len(token_ids)\n",
    "    seq_len=seq_len-4\n",
    "    for i in range(0, total_tokens - seq_len, seq_len):\n",
    "        input_ids = token_ids[i : i + seq_len]\n",
    "        wrapped_seq=prefix_tokens+input_ids.tolist()+suffix_tokens\n",
    "        wrapped_tensor=torch.tensor(wrapped_seq, dtype=torch.long)\n",
    "        examples.append(wrapped_tensor)\n",
    "    input_ids_tensor = torch.stack(examples)\n",
    "    return input_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NUM_SEQUENCES = 130351\n",
    "input_ids = create_dataset_from_tokens(all_token_ids, seq_len=SEQ_LEN)\n",
    "input_ids = input_ids[:TARGET_NUM_SEQUENCES]\n",
    "print(f\"Total sequences: {len(input_ids)}\")\n",
    "N = len(input_ids)\n",
    "num_subsets = N // SUBSET_SIZE\n",
    "print(num_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_kl_loss(probe_logits, final_logits, top_k=100, eps=1e-8):\n",
    "    B, S, V = final_logits.size()\n",
    "\n",
    "    _, topk_final = torch.topk(final_logits, top_k, dim=-1)\n",
    "    _, topk_probe = torch.topk(probe_logits, top_k, dim=-1)\n",
    "\n",
    "    combined = torch.cat([topk_final, topk_probe], dim=-1).view(-1, 2 * top_k)  # shape: (B*S, 2*k)\n",
    "\n",
    "    max_len = combined.size(-1)\n",
    "    unique_list = []\n",
    "    mask_list = []\n",
    "    for idxs in combined:\n",
    "        unique = torch.unique(idxs, sorted=True)\n",
    "        # Pad with the last valid index if shorter than max_len\n",
    "        if unique.size(0) < max_len:\n",
    "            padded = torch.cat([unique, unique[-1].repeat(max_len - unique.size(0))])\n",
    "        else:\n",
    "            padded = unique[:max_len]\n",
    "        unique_list.append(padded)\n",
    "        # Create mask to mark valid indices\n",
    "        mask_list.append(torch.arange(max_len, device=unique.device) < unique.size(0))\n",
    "\n",
    "    union_indices = torch.stack(unique_list).view(B, S, max_len)\n",
    "    valid_mask = torch.stack(mask_list).view(B, S, max_len)  \n",
    "\n",
    "\n",
    "    probe_subset_logits = torch.gather(probe_logits, 2, union_indices).float()\n",
    "    final_subset_logits = torch.gather(final_logits, 2, union_indices).float()\n",
    "\n",
    "    target_probs = F.softmax(final_subset_logits, dim=-1).clamp(min=eps)\n",
    "    target_log_probs = torch.log(target_probs)\n",
    "    pred_log_probs = F.log_softmax(probe_subset_logits, dim=-1)\n",
    "\n",
    "    # Compute KL divergence elements and mask out padded positions\n",
    "    kl_elements = target_probs * (target_log_probs - pred_log_probs) * valid_mask.float()\n",
    "\n",
    "    # Sum over vocabulary subset dimension\n",
    "    kl_per_token = kl_elements.sum(dim=-1)\n",
    "\n",
    "    # Compute mean only over valid tokens (positions where any valid vocab exists)\n",
    "    # This prevents division by zero and excludes padded positions properly\n",
    "    token_mask = (valid_mask.any(dim=-1)).float()  # shape (B, S)\n",
    "    loss = kl_per_token.sum() / (token_mask.sum() + eps)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(batch_input):\n",
    "    with torch.no_grad():\n",
    "        hooks_to_cache = [f'blocks.{i}.hook_resid_post' for i in range(n_layers)] \n",
    "\n",
    "        logits, cache = model.run_with_cache(batch_input, names_filter=hooks_to_cache)\n",
    "        #print(cache[f'blocks.0.hook_resid_post'].dtype)\n",
    "        resid_post_outs = [cache[f'blocks.{i}.hook_resid_post'] for i in range(n_layers)]\n",
    "\n",
    "    return resid_post_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logits_from_resid(resid, soft_cap=30.0):\n",
    "\n",
    "    normed = ln_final(resid)\n",
    "\n",
    "    W_U_casted = W_U.to(normed.device).type_as(normed)\n",
    "    b_U_casted = b_U.to(normed.device).type_as(normed)\n",
    "    logits = torch.einsum('bsd,dk->bsk', normed, W_U_casted) + b_U_casted\n",
    "\n",
    "    logits = soft_cap * torch.tanh(logits / soft_cap)#done in gemma 2\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model, bias=True)\n",
    "        torch.nn.init.xavier_normal_(self.linear.weight)\n",
    "        torch.nn.init.zeros_(self.linear.bias)            \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start GPU monitoring in background\n",
    "#the interval is in seconds, adjust as needed\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def monitor_gpu(interval=10):\n",
    "    while True:\n",
    "        try:\n",
    "            output = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "            print(\"\\n[GPU USAGE]\")\n",
    "            print(output)\n",
    "        except Exception as e:\n",
    "            print(\"[nvidia-smi error]:\", e)\n",
    "        time.sleep(interval)\n",
    "\n",
    "monitor_thread = threading.Thread(target=monitor_gpu, args=(15,), daemon=True)\n",
    "monitor_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore this, just to clear memory if you want to run loop again\n",
    "for var_name in ['post_outs', 'final_logits', 'post_logits', 'post_probes','new_probes', 'resid_post_outs', 'subset_input', 'optimizers']:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "    elif var_name in locals():\n",
    "        del locals()[var_name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_cap_value=30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset_idx in range(num_subsets):    \n",
    "    start = subset_idx * SUBSET_SIZE\n",
    "    end = start + SUBSET_SIZE\n",
    "    print(f\"\\n--- Processing subset {subset_idx+1} / {num_subsets} -- Sequences {start} to {end-1}\")\n",
    "\n",
    "    subset_input = input_ids[start:end].to(DEVICE)\n",
    "    # Extract activations for this subset\n",
    "    resid_post_outs = get_activations(subset_input)\n",
    "    # Move activations to CPU to save GPU memory (keep as FP16 for storage efficiency)\n",
    "    torch.save(resid_post_outs, f'activations/activations_{subset_idx + 1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.state_dict()['unembed.W_U'].to(DEVICE).clone()\n",
    "b_U = model.state_dict()['unembed.b_U'].to(DEVICE).clone()\n",
    "\n",
    "import copy\n",
    "ln_final = copy.deepcopy(model.ln_final).to(DEVICE)\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_MULTIPLIER = 2 # Set to 2 for 2 subsets at once, or 3 for 3, etc.\n",
    "num_combined_batches = (num_subsets + BATCH_MULTIPLIER - 1) // BATCH_MULTIPLIER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize one probe per layer (FP32 on DEVICE)\n",
    "probes = [Probe(d_model).to(DEVICE).to(torch.float32) for _ in range(n_layers)]\n",
    "optimizers = [torch.optim.AdamW(probe.parameters(), lr=1e-5) for probe in probes]\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    epoch_loss_per_layer = [0.0 for _ in range(n_layers)]\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        probe = probes[layer]\n",
    "        opt = optimizers[layer]\n",
    "\n",
    "        for combined_idx in range(num_combined_batches):\n",
    "            start_subset = combined_idx * BATCH_MULTIPLIER + 1\n",
    "            end_subset = min(start_subset + BATCH_MULTIPLIER - 1, num_subsets)\n",
    "\n",
    "            # Gather and concatenate only this layer's activations for all needed subsets\n",
    "            activ_list = []\n",
    "            final_layer_activ_list = []\n",
    "            for subset_idx in range(start_subset, end_subset + 1):\n",
    "                acts = torch.load(f'activations/activations_{subset_idx}.pth', map_location='cpu')\n",
    "                activ_list.append(acts[layer])\n",
    "                # Use the last layer’s activations as the reference for logits\n",
    "                final_layer_activ_list.append(acts[-1])\n",
    "\n",
    "            activ_batch = torch.cat(activ_list, dim=0).to(DEVICE).to(torch.float32)\n",
    "            final_layer_activ_batch = torch.cat(final_layer_activ_list, dim=0).to(DEVICE).to(torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                final_logits_batch = compute_logits_from_resid(final_layer_activ_batch, soft_cap=30.0)\n",
    "\n",
    "            out = probe(activ_batch)\n",
    "            normed_out = ln_final(out)\n",
    "            B, S, d_model_ = normed_out.shape\n",
    "            normed_out_flat = normed_out.view(-1, d_model_)\n",
    "\n",
    "            W_U_casted = W_U.to(normed_out_flat.dtype)\n",
    "            b_U_casted = b_U.to(normed_out_flat.dtype)\n",
    "            logits_flat = torch.nn.functional.linear(normed_out_flat, W_U_casted.t().contiguous(), b_U_casted)\n",
    "            probe_logits = logits_flat.view(B, S, -1)\n",
    "            probe_logits = soft_cap_value * torch.tanh(probe_logits / soft_cap_value)\n",
    "\n",
    "            loss = stable_kl_loss(probe_logits, final_logits_batch)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(probe.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss_per_layer[layer] += loss.item()\n",
    "\n",
    "            # Cleanup\n",
    "            del activ_batch, final_layer_activ_batch, out, normed_out, normed_out_flat\n",
    "            del logits_flat, probe_logits, loss, final_logits_batch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Average loss per layer:\")\n",
    "    for layer in range(n_layers):\n",
    "        avg_loss = epoch_loss_per_layer[layer] / num_batches\n",
    "        print(f\"  Layer {layer}: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize one probe per layer (FP32 on DEVICE)\n",
    "probes = [Probe(d_model).to(DEVICE).to(torch.float32) for _ in range(n_layers)]\n",
    "optimizers = [torch.optim.AdamW(probe.parameters(), lr=1e-5) for probe in probes]\n",
    "\n",
    "num_batches = num_subsets\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    epoch_loss_per_layer = [0.0 for _ in range(n_layers)]\n",
    "    \n",
    "    for combined_idx in range(num_combined_batches):\n",
    "        start_subset = combined_idx * BATCH_MULTIPLIER + 1\n",
    "        end_subset = min(start_subset + BATCH_MULTIPLIER - 1, num_subsets)\n",
    "    \n",
    "        # For each layer (and final layer), build list of tensors to concatenate\n",
    "        batch_resid_post_outs = [[] for _ in range(n_layers)]\n",
    "\n",
    "        # Load each subset activation and append per-layer activations\n",
    "        for subset_idx in range(start_subset, end_subset + 1):\n",
    "            acts = torch.load(f'activations/activations_{subset_idx}.pth', map_location='cpu')\n",
    "            for layer_idx in range(n_layers):\n",
    "                batch_resid_post_outs[layer_idx].append(acts[layer_idx])\n",
    "                \n",
    "        print(\"Shapes of all tensors in last layer before concatenation:\")\n",
    "        for i, t in enumerate(batch_resid_post_outs[-1]):\n",
    "            print(f\"  Tensor {i}: {t.shape}\")\n",
    "\n",
    "        batch_resid_post_outs = [\n",
    "            torch.cat(layer_acts_list, dim=0) for layer_acts_list in batch_resid_post_outs\n",
    "        ]\n",
    "\n",
    "        for layer in range(n_layers):\n",
    "            probe = probes[layer]\n",
    "            opt = optimizers[layer]\n",
    "\n",
    "            activ_batch = batch_resid_post_outs[layer].to(DEVICE).to(torch.float32)     # activations for this layer\n",
    "            final_layer_activ_batch = batch_resid_post_outs[-1].to(DEVICE).to(torch.float32)  # final layer activations\n",
    "\n",
    "            with torch.no_grad():\n",
    "                print(final_layer_activ_batch.shape)\n",
    "                final_logits_batch = compute_logits_from_resid(final_layer_activ_batch, soft_cap=30.0)\n",
    "\n",
    "            out = probe(activ_batch)\n",
    "            normed_out = ln_final(out)#using the stored ln_final\n",
    "            B, S, d_model_ = normed_out.shape\n",
    "            normed_out_flat = normed_out.view(-1, d_model_)\n",
    "    \n",
    "            # Before using W_U and b_U in the loop:\n",
    "            W_U_casted = W_U.to(normed_out_flat.dtype)\n",
    "            b_U_casted = b_U.to(normed_out_flat.dtype)\n",
    "            logits_flat = torch.nn.functional.linear(normed_out_flat, W_U_casted.t().contiguous(), b_U_casted)\n",
    "            probe_logits = logits_flat.view(B, S, -1)\n",
    "            probe_logits = soft_cap_value * torch.tanh(probe_logits / soft_cap_value)\n",
    "        \n",
    "            loss = stable_kl_loss(probe_logits, final_logits_batch)\n",
    "    \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(probe.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "    \n",
    "            epoch_loss_per_layer[layer] += loss.item()\n",
    "    \n",
    "                # Cleanup\n",
    "            del activ_batch, final_layer_activ_batch, out, normed_out, normed_out_flat\n",
    "            del logits_flat, probe_logits, loss, final_logits_batch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Average loss per layer:\")\n",
    "    for layer in range(n_layers):\n",
    "        avg_loss = epoch_loss_per_layer[layer] / num_batches\n",
    "        print(f\"  Layer {layer}: {avg_loss:.4f}\")\n",
    "\n",
    "# Save final trained probes (one per layer) after all epochs\n",
    "print(\"\\nSaving final trained probes per layer...\")\n",
    "for layer in range(n_layers):\n",
    "    save_dir='probes'\n",
    "    save_path = f'{save_dir}/probe_{layer}.pt'\n",
    "    torch.save(probes[layer].state_dict(), save_path)\n",
    "    print(f\"Saved layer {layer} final probe to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
